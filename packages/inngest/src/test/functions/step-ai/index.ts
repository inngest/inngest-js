import { inngest } from "../client";
import { unknown } from "zod";

export type AiRequest = {
  /**
   * The model used for generating the response, e.g., "gpt-4".
   */
  model: "gpt-4";

  /**
   * An array of message objects representing the conversation so far, typically including the user's prompt and possibly system instructions.
   */
  messages: {
    /**
     * The role of the message sender. Can be "system", "user", or "assistant".
     */
    role: "system" | "user" | "assistant";

    /**
     * The actual content of the message. This is the text input or instruction.
     */
    content: string;
  }[];

  /**
   * Controls the randomness of the output. A higher temperature (closer to 1) makes the output more creative, while a lower value (closer to 0) makes it more deterministic.
   */
  temperature?: number;

  /**
   * The maximum number of tokens to generate in the response. Token count includes both input and output tokens.
   */
  maxTokens?: number;
};

export type AiResponse = {
  /**
   * The unique identifier for the API response.
   */
  id: string;

  /**
   * The type of object, which is typically "chat.completion" for completion responses.
   */
  object: string;

  /**
   * A Unix timestamp indicating when the response was created.
   */
  created: number;

  /**
   * The model used to generate the response (e.g., "gpt-4").
   */
  model: string;

  /**
   * An array of choices returned by the model. Typically, there is only one choice.
   */
  choices: {
    /**
     * The index of this particular choice in the list of choices.
     */
    index: number;

    /**
     * The message content generated by the assistant.
     */
    message: {
      /**
       * The role of the message sender. For the assistant's response, this will be "assistant".
       */
      role: "assistant";

      /**
       * The actual response text/content generated by the model.
       */
      content: string;
    };

    /**
     * The reason the model stopped generating a response. Typically "stop" when it completes normally.
     */
    finishReason: string;
  }[];

  /**
   * A summary of token usage for this request.
   */
  usage: {
    /**
     * The number of tokens used for the input prompt.
     */
    promptTokens: number;

    /**
     * The number of tokens used for the completion (response).
     */
    completionTokens: number;

    /**
     * The total number of tokens used for both input and completion.
     */
    totalTokens: number;
  };
};

export const sampleAiRequest = {
  model: "gpt-4" as AiRequest["model"],
  messages: [{ role: "system" as const, content: "this is a test" }],
  temperature: 0.5,
  maxTokens: 2400,
};

export default inngest.createFunction(
  { id: "step-ai" },
  { event: "demo/step.ai" },
  async ({ step }) => {
    await step.ai(
      "generateText",
      (req: AiRequest) => {
        return {
          id: "123",
          object: "chat.completion",
          created: Date.now(),
          model: req.model,
          choices: [
            {
              index: 0,
              message: {
                role: "system",
                content: "Hello, how can I help you today?",
              },
              finishReason: "stop",
            },
          ],
          usage: {
            promptTokens: 0,
            completionTokens: 0,
            totalTokens: 0,
          },
        };
      },
      sampleAiRequest
    );
  }
);
